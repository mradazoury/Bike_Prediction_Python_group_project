{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from My_Functions import * \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score,roc_curve\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict,validation_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer, RobustScaler,PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator , MultipleLocator\n",
    "##from matplotlib import XAxis\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer,MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression, OrthogonalMatchingPursuit\n",
    "from sklearn.model_selection import train_test_split , TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import plotly.tools as tls\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='Furqan92', api_key='22DfVN5rFRg79OYygN5h')\n",
    "import plotly.plotly as py\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas import DataFrame \n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "random_seed = 504\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "wd_h=pd.read_csv('./workingdays_data_prepared.csv')\n",
    "#we_h=pd.read_csv('./weekends_holi_data_prepared.csv')\n",
    "wd_h['dteday']=pd.to_datetime(wd_h['dteday'], format='%Y-%m-%d')\n",
    "#we_h['dteday']=pd.to_datetime(we_h['dteday'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset 2011 -> 2012Q3 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing the dataset in one year and a half and Q3 iun order to be able to train level 0 AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_2011_2012Q2 = wd_h[wd_h['dteday'] < datetime.datetime(2012, 7, 5, 0, 0)].drop(['cnt','casual','registered','dteday'],axis=1)## NONE OF THst_EM IN DATA\n",
    "Y_cnt_train_2011_2012Q2 =wd_h['cnt'][wd_h['dteday'] < datetime.datetime(2012, 7, 5, 0, 0)]\n",
    "\n",
    "X_Test_2012Q3 = wd_h[(wd_h['dteday'] >= datetime.datetime(2012, 7, 5, 0, 0)) & (wd_h['dteday'] <= datetime.datetime(2012, 9, 30, 0, 0))].drop(['cnt','casual','registered','dteday'],axis=1)## NONE OF THEM IN DATA\n",
    "Y_cnt_test_2012Q3 =wd_h['cnt'][(wd_h['dteday'] >= datetime.datetime(2012, 7, 5, 0, 0)) & (wd_h['dteday'] <= datetime.datetime(2012, 9, 30, 0, 0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression w/ GS for *Weekdays* for 2011 -> 2012Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the simplest model on Q3 , even though the score is high here this is the prediction for Q3 (FALL) , as we saw in the EDA FALL SPRING AND SUMMER behaved in the same way but winter was a bit different so we can say that the model learned from 5 similar season and we would not expect the same score for Q4 ( winter ) . This clearly an indicator that the feature created previously have helped our model to learn more about the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408507786648361"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Linear Regression\n",
    "lm_parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True]}\n",
    "\n",
    "lm = GridSearchCV(LinearRegression(),\n",
    "                                 param_grid=lm_parameters,\n",
    "                                 cv=tscv,return_train_score=True)\n",
    "\n",
    "lm.fit(X_Train_2011_2012Q2, Y_cnt_train_2011_2012Q2)\n",
    "lm.cv_results_\n",
    "lm_predictions = lm.predict(X_Test_2012Q3)\n",
    "lm.score(X_Test_2012Q3, Y_cnt_test_2012Q3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest w/ GS for *Weekdays* for 2011 -> 2012Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying linear regressor we tried random forest with a large grid search in order to make sure that we are using the best possible parameters for the model \n",
    "Our expectations were to have a higher score then the linear model but we did not, This could be due to the fact that there are alot of linearly correlated target to the data such as the new weather variable created and the \n",
    "mean_per_hour which reflects a direct linear correlation of the past target in the same hour.\n",
    "Random forest acts best when the data is clearly partitione in groups such as the Rush_Hour and Is_Daylight that clearly differentiate the different target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9289808963621973"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_parameters = {'n_estimators': [10, 30 ,100],\n",
    "                                             'bootstrap': [True],\n",
    "                                             'max_depth': [80, 100 ],\n",
    "                                             'max_features': ['sqrt',16 ,32],\n",
    "                                             'min_samples_leaf': [2,  5 , 8],\n",
    "                                             'min_samples_split': [ 10 , 8 , 15],\n",
    "                                            'random_state':[random_seed],\n",
    "                                            'criterion':['mse']}\n",
    "rf = GridSearchCV(RandomForestRegressor(),\n",
    "                                 param_grid= RF_parameters,\n",
    "                                 cv=tscv)\n",
    "\n",
    "rf.fit(X_Train_2011_2012Q2, Y_cnt_train_2011_2012Q2)\n",
    "rf.cv_results_\n",
    "rf_predictions = lm.predict(X_Test_2012Q3)\n",
    "rf.score(X_Test_2012Q3, Y_cnt_test_2012Q3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors regressor w/ GS for *Weekends* for 2011 -> 2012Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After trying the basic model we went into the more exotic ones , here the KNN claculate the difference between the differernt point and tries to classsif them \n",
    "in groups and then compare these groups to the target and trie to get teh cclosest estimated number.\n",
    "The bases of trying this model and why it seemed like it would make sense to us is because in our EDA it looked like there are multiple waya to cluster the differnret hours \n",
    "either by weather , hr ,season or even day  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9211277733616634"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_parameters = {'n_neighbors':[5,10,15],'weights':['uniform'], 'algorithm':['auto'],'leaf_size':[10, 20,30],\n",
    "                  'p':[1,2,3],'metric':['minkowski']}\n",
    "knn = GridSearchCV(KNeighborsRegressor(),\n",
    "                                 param_grid=knn_parameters,\n",
    "                                 cv=tscv,return_train_score=True)\n",
    "knn.fit(X_Train_2011_2012Q2, Y_cnt_train_2011_2012Q2)\n",
    "# rf.cv_results_\n",
    "knn_predictions = knn.predict(X_Test_2012Q3)\n",
    "knn.score(X_Test_2012Q3, Y_cnt_test_2012Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector regressor w/ GS for *Weekends* for 2011 -> 2012Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This algorithm is known for it complexity and the amount of time it takes oto run but we wanted to give it a try , so instead of using a wide range for the gridsearc we just used one of the basic kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7455187270287198"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=tscv,\n",
    "                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n",
    "                               \"gamma\": np.logspace(-2, 2, 5)})\n",
    "\n",
    "svr.fit(X_Train_2011_2012Q2, Y_cnt_train_2011_2012Q2)\n",
    "# rf.cv_results_\n",
    "svr_predictions = svr.predict(X_Test_2012Q3)\n",
    "svr.score(X_Test_2012Q3, Y_cnt_test_2012Q3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost regressor w/ GS for *Weekends* for 2011 -> 2012Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed: 30.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9430402871403295"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "param_grid = {'learning_rate': [0.01, 0.1], \n",
    "          'max_depth': [4,8,12],\n",
    "          'min_child_weight': [3,5,10,20,35,50],\n",
    "          'subsample': [0.5, 0.75],\n",
    "          'colsample_bytree': [0.5, 0.75],\n",
    "          'n_estimators': [100, 300], 'random_state':[random_seed]\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "xg = GridSearchCV(model,\n",
    "                    param_grid = param_grid,\n",
    "                    cv = tscv,\n",
    "                    n_jobs = -1,\n",
    "                    scoring = 'explained_variance',\n",
    "                    verbose=True)\n",
    "xg.fit(X_Train_2011_2012Q2, Y_cnt_train_2011_2012Q2)\n",
    "xg_predictions = xg.predict(X_Test_2012Q3)\n",
    "xg.score(X_Test_2012Q3, Y_cnt_test_2012Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking predictions from LR and Random Forest and xgboost to test on 2012Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR and Random Forest predictions\n",
    "combinedPredictions = pd.DataFrame({'lm_predictions':lm_predictions,'rf_predictions':rf_predictions,'xg_predictions':xg_predictions })\n",
    "\n",
    "# Concat original data\n",
    "X_Test_2012Q3.reset_index(drop=True,inplace=True)\n",
    "combinedPredictions = pd.concat([combinedPredictions,X_Test_2012Q3], axis=1)\n",
    "\n",
    "# Target data\n",
    "combinedPredictionsTarget = pd.DataFrame({'target':Y_cnt_test_2012Q3})\n",
    "\n",
    "#Reset indices\n",
    "combinedPredictions.reset_index(drop=True,inplace=True)\n",
    "combinedPredictionsTarget.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Q4 data and concat it with its level-0 predctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Q4 data\n",
    "X_Test_Q4 = wd_h[(wd_h['dteday'] >= datetime.datetime(2012, 10, 1, 0, 0)) & (wd_h['dteday'] <= datetime.datetime(2012, 12, 31, 0, 0))].drop(['cnt','casual','registered','dteday'],axis=1)## NONE OF THEM IN DATA\n",
    "Y_cnt_test_Q4 =wd_h['cnt'][(wd_h['dteday'] >= datetime.datetime(2012, 10, 1, 0, 0)) & (wd_h['dteday'] <= datetime.datetime(2012, 12, 31, 0, 0))]\n",
    "oringal_cnt = Y_cnt_test_Q4\n",
    "\n",
    "#get level-0 predictions for Q4 data\n",
    "Q4_lm_predictions = pd.DataFrame(lm.predict(X_Test_Q4), columns=[\"lm_predictions\"])\n",
    "Q4_rf_predictions = pd.DataFrame(rf.predict(X_Test_Q4), columns=[\"rf_predictions\"])\n",
    "Q4_xg_predictions = pd.DataFrame(xg.predict(X_Test_Q4), columns=[\"xg_predictions\"])\n",
    "\n",
    "X_Test_Q4.reset_index(drop=True,inplace=True)\n",
    "Q4_lm_predictions.reset_index(drop=True,inplace=True)\n",
    "Q4_rf_predictions.reset_index(drop=True,inplace=True)\n",
    "Q4_xg_predictions.reset_index(drop=True,inplace=True)\n",
    "Y_cnt_test_Q4.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#concat Q4 with level-0 predictions \n",
    "X_Test_Q4_with_predictions = pd.concat([Q4_lm_predictions, Q4_rf_predictions,Q4_xg_predictions, X_Test_Q4 ], axis=1)\n",
    "Y_cnt_test_Q4 = pd.DataFrame({\"target\": Y_cnt_test_Q4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8839966859540709"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(X_Test_Q4, Y_cnt_test_Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8715926918607073"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_Test_Q4, Y_cnt_test_Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.900164954326128"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg.score(X_Test_Q4, Y_cnt_test_Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8340477314724115"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_Test_Q4, Y_cnt_test_Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7077025873956615"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.score(X_Test_Q4, Y_cnt_test_Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost regressor w/ GS for Level 1 *Weekends* for 2011 -> 2012Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 454 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 804 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1254 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9022288970285075"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'learning_rate': [0.01, 0.1], \n",
    "          'max_depth': [4,8,12],\n",
    "          'min_child_weight': [3,5,10,20,35,50],\n",
    "          'subsample': [0.5, 0.75],\n",
    "          'colsample_bytree': [0.5, 0.75],\n",
    "          'n_estimators': [100, 300], 'random_state':[random_seed]\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "xg1 = GridSearchCV(model,\n",
    "                    param_grid = param_grid,\n",
    "                    cv = tscv,\n",
    "                    n_jobs = -1,\n",
    "                    scoring = 'explained_variance',\n",
    "                    verbose=True)\n",
    "\n",
    "xg1.fit(combinedPredictions, combinedPredictionsTarget)\n",
    "xg1_predictions = xg1.predict(X_Test_Q4_with_predictions)\n",
    "xg1.score(X_Test_Q4_with_predictions, Y_cnt_test_Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg1_predictions  = pd.concat( (pd.DataFrame(xg1.predict(X_Test_Q4_with_predictions)), Y_cnt_test_Q4), axis =1 )\n",
    "xg1_predictions.to_csv(\"pred_working.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
